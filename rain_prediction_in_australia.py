# -*- coding: utf-8 -*-
"""Rain_Prediction_in_Australia.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1rIy7Ojdtb0pkz3bKhxqyinB7q6vr70Yc

**20L-1049, 20L-1222, 20L-0914**
"""

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.impute import SimpleImputer
from sklearn.preprocessing import LabelEncoder
from sklearn.preprocessing import MinMaxScaler
from sklearn.preprocessing import StandardScaler

"""**Loading Data**"""

df = pd.read_csv('weatherAUS.csv')

"""**Data Cleaning**"""

df.isnull().sum()

df.dropna(subset=["RainToday","RainTomorrow"],inplace=True)

num_cols = df.select_dtypes(include=np.number).columns

se = SimpleImputer()

# Apply the imputer to all numeric columns
df[num_cols] = se.fit_transform(df[num_cols])

# Check for missing values after imputation
missing_values = df.isnull().sum()

print(missing_values)

columns=['MinTemp', 'MaxTemp','Evaporation', 'Sunshine',
         'WindGustSpeed', 'WindSpeed9am', 'WindSpeed3pm', 'Humidity9am', 'Humidity3pm',
        'Pressure9am', 'Pressure3pm', 'Cloud9am', 'Cloud3pm', 'Temp9am',
        'Temp3pm']

for column in columns:
  # Find 1st and 3rd quartile values
  Q1 = df[column].quantile(0.25)
  Q3 = df[column].quantile(0.75)
  IQR = Q3 - Q1
  # Find upper and lower limit of the column
  lower_limit = Q1 - 1.5 * IQR
  upper_limit = Q3 + 1.5 * IQR
  # Replace outliers with the mean of the column
  df[column] = df[column].apply(lambda x: df[column].mean() if x < lower_limit or x > upper_limit else x)
  print(column, ((df[column]>upper_limit) | (df[column]<lower_limit)).sum())

#remove duplicates
df.drop_duplicates(inplace=True)

"""**Data Transformation**"""

le=LabelEncoder()
#Replace yes and no with 1 and 0 respectively
df["RainToday"]=le.fit_transform(df["RainToday"])
df["RainTomorrow"]=le.fit_transform(df["RainTomorrow"])

df.head()

"""**Dimensionality Reduction**"""

# Drop the Date column since it is of no use
df.drop(['Date'], axis=1, inplace=True)

df.dtypes

"""**Normalization and Standardization**"""

# from sklearn.preprocessing import MinMaxScaler
# scaler=MinMaxScaler()
# df=scaler.fit_transform(df)

# from sklearn.preprocessing import StandardScaler
# scaler=StandardScaler()
# df=scaler.fit_transform(df)

#standarization and normalization are not required in our dataz

"""**Data Validity**"""

print("null values in dataset: ", df[df.select_dtypes(include=np.number).columns].isnull().sum())

df_num=df.select_dtypes(include=["float64"])
plt.figure(figsize=(15, 12))
for n, i in enumerate(df_num):
    # add a new subplot iteratively
    ax = plt.subplot(6, 3, n + 1)
    plt.subplots_adjust(hspace=0.7)
    plt.suptitle("Outliers", fontsize=18, y=0.95)

    # filter df and plot on the new subplot axis
    sns.boxplot(df_num[i],palette="rocket",orient="v",width=0.7,linewidth=4,ax=ax)

    # chart formatting
    ax.set_title(i.upper())
    ax.set_xlabel("")

#since there are no null values in significant columns and no outliers, data is valid...

"""**New CSV File**"""

# Saving data to another csv
df.to_csv('Australia_dataset.csv')